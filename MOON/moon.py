# Li, Q., He, B., Song, D., 2021a. Model-contrastive federated learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713â€“10722.

import random
import os
import scipy.io as io
from tqdm import tqdm
from MOON.options import parse
from MOON.dataset import Load_Data
from MOON.model import Net
from MOON.weight_avg import W_A
import torch
import numpy as np
import copy
import torch.nn as nn

# init
argv = parse()

def acc(Label, Pred):
    true_list = np.array([Label[i] - Pred[i] for i in range(len(Label))])
    true_list[true_list != 0] = 1
    err = true_list.sum()
    acc = (len(Label)-err)/len(Label)
    return acc

# Set seed and device
torch.manual_seed(argv.seed)
np.random.seed(argv.seed)
random.seed(argv.seed)
if torch.cuda.is_available():
    device = torch.device("cuda")
    torch.cuda.manual_seed_all(argv.seed)
else:
    device = torch.device("cpu")


# dataset
# Here, we take three sites as an example, but in practice, any number of sites is allowed.
# Please refer to the demo data and place your data files and label files for each site in their respective folders.
data_dir = argv.data_dir
label_dir = argv.label_dir
data_file = os.listdir(data_dir)
label_file = os.listdir(label_dir)
site_num = len(data_file)
sample_num_list = []
for num in range(site_num):
    data_path = data_dir + '/' + data_file[num]
    label_path = label_dir + '/' + label_file[num]
    exec ("dataset%s=Load_Data(data_path, label_path, k_fold=argv.k_fold)" % num)
    exec ("dataset_test%s=Load_Data(data_path, label_path, k_fold=argv.k_fold)" % num)
    exec ("sample_num_list.append(len(dataset%s))" % num)
    exec ("dataloader%s=torch.utils.data.DataLoader(dataset%d, batch_size=argv.minibatch_size, shuffle=False)" % (num,num))
    exec ("dataloader_test%s=torch.utils.data.DataLoader(dataset_test%d, batch_size=1, shuffle=False)" % (num, num))


# fl train
cos = nn.CosineSimilarity(dim=-1) # cosine similarity

# server
# You can replace the model as needed.
# You are free to decide here whether to use the pre-trained parameters we provide.
model_init = Net(input_dim=116, hidden_dim=64, num_classes=2)
parameter_init = model_init.state_dict()
parameter_init_fix = model_init.state_dict()

R = 0.0
for k in range(argv.k_fold):
    R_k = []
    for i in range(site_num):
        exec('L%s=[]' % i)

    # train
    for iter_num in range(argv.num_iters):
        para_list = []
        for site in range(site_num):
            exec('dataset%s.set_fold(k,train=True)' % site)
            exec("dataloader=dataloader%s" % site)
            # current round local model
            model = Net(input_dim=116, hidden_dim=64, num_classes=2)
            # last round of local models
            model_prev = Net(input_dim=116, hidden_dim=64, num_classes=2)
            # global model
            model_global = Net(input_dim=116, hidden_dim=64, num_classes=2)
            model.to(device)
            model_prev.to(device)
            model_global.to(device)
            model.load_state_dict(parameter_init)
            if iter_num != 0:
                model_prev.load_state_dict(prev_para[site])
                model_prev.eval()
                model_global.load_state_dict(parameter_init)
                model_global.eval()
            criterion = torch.nn.CrossEntropyLoss() # cross-entropy loss
            optimizer = torch.optim.Adam(model.parameters(), lr=argv.lr)

            for epoch in range(argv.num_epochs):
                Label = []
                Pred = []
                loss_accumulate = 0.0
                for i, x in enumerate(tqdm(dataloader, ncols=80, desc=f'k:{k},i:{iter_num},s:{site},e:{epoch}')):
                    data = x['X'].permute(0, 2, 1)
                    label = x['y']
                    Label.extend(label.tolist())

                    local_feat, out = model(data.to(device)) # local_feat: local model feature for this round, out: prediction score

                    if iter_num == 0:
                        loss = criterion(out, label.to(device))
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        pred = out.argmax(1)
                        prob = out.softmax(1)
                        Pred.extend(pred.tolist())
                        loss_accumulate += loss.detach().cpu().numpy()

                    if iter_num != 0:
                        global_feat,_ = model_global(data.to(device))
                        sim_z_global = cos(local_feat, global_feat) # cosine similarity between local and global model features
                        logits = sim_z_global.reshape(-1, 1)
                        prev_feat,_ = model_prev(data.to(device))
                        # cosine similarity between features generated by the local model in the current round and in the previous round
                        sim_z_prev = cos(local_feat, prev_feat)
                        logits = torch.cat((logits, sim_z_prev.reshape(-1,1)), dim=1)
                        loss_c = 0.0001 * criterion(logits, torch.zeros(len(label)).to(device).long())
                        loss = criterion(out, label.to(device))
                        loss = loss+loss_c
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                        pred = out.argmax(1)
                        prob = out.softmax(1)
                        Pred.extend(pred.tolist())
                        loss_accumulate += loss.detach().cpu().numpy()

                exec('L%s.append(loss_accumulate)' % site)
                Acc = acc(Label, Pred)
                print('acc:', Acc)

            # save model
            if iter_num == argv.num_iters-1:
                torch.save(model.state_dict(), argv.save_dir + '/' + 'site' + str(site) + '_fold' + str(k) + '_model.pth')
            para_list.append(model.state_dict())
        prev_para = copy.deepcopy(para_list)

        # federated aggregation
        parameter_init = W_A(para_list, sample_num_list)

        # test
        for site in range(site_num):
            exec('dataset_test%s.set_fold(k,train=False)' % site)
            exec("dataloader_test=dataloader_test%s" % site)
            model = Net(input_dim=116, hidden_dim=64, num_classes=2)
            model.load_state_dict(parameter_init)
            model.eval()
            Label = []
            Pred = []
            if iter_num == argv.num_iters - 1: Prob = []
            for i, x in enumerate(tqdm(dataloader_test, ncols=80, desc=f'k:{k},i:{iter_num},s:{site}')):
                data = x['X'].permute(0, 2, 1)
                label = x['y']
                Label.extend(label.tolist())
                local_feat, out = model(data)
                pred = out.argmax(1)
                prob = out.softmax(1)
                Pred.extend(pred.tolist())
                if iter_num == argv.num_iters - 1: Prob.append(prob.tolist())
            Acc = acc(Label, Pred)
            print('acc:', Acc)
            print('k:', k, 'i:', iter_num, 's:', site, 'result:')
            print(Acc)
            R_k.append(Acc)

            # save results
            if iter_num == argv.num_iters - 1:
                Label = np.array(Label)
                Pred = np.array(Pred)
                Prob = torch.tensor(Prob).squeeze(1).numpy()
                np.save(argv.save_dir + '/' + 'site' + str(site) + '_fold' + str(k) + '_Label', Label)
                np.save(argv.save_dir + '/' + 'site' + str(site) + '_fold' + str(k) + '_Pred', Pred)
                np.save(argv.save_dir + '/' + 'site' + str(site) + '_fold' + str(k) + '_Prob', Prob)

    # save fold results
    R_k = np.array(R_k).reshape(argv.num_iters, site_num)
    io.savemat(argv.save_dir + '/' + 'fold' + str(k) + '_result.mat', {'acc': R_k})

    # save loss
    for i in range(site_num):
        exec('L%s=np.array(L%d)' % (i, i))
        exec("np.save(argv.save_dir + '/' + 'site' + str(i) + '_fold' + str(k) + '_Loss', L%s)" % i)
    print('fold ', str(k), 'result:')
    print(R_k)
    R += R_k
    parameter_init = parameter_init_fix

print('final result:')
print(R/argv.k_fold)

# save final result
io.savemat(argv.save_dir + '/' + 'final_result.mat', {'acc': R / argv.k_fold})
